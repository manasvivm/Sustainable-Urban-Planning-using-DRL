# env
objectives_plan: objectives_hlg
init_plan: init_plan_hlg

# reward
reward_specs:
  road_network_weight: 0.0
  life_circle_weight: 8.0
  greenness_weight: 1.0
  wastemgmt_weight: 1.0
  traffic_line_weight: 1.0

# agent
agent_specs:
  batch_stage: false

# training parameters
skip_land_use: false
skip_road: true
road_ratio: 0.00
gamma: 2.0
tau: 0.0
state_encoder_specs:
  state_encoder_hidden_size: [64, 16]
  gcn_node_dim: 8
  num_gcn_layers: 4
  num_edge_fc_layers: 1
  max_num_nodes: 5000
  max_num_edges: 5000
  num_attention_heads: 2
policy_specs:
  policy_land_use_head_hidden_size: [32, 1]
  policy_road_head_hidden_size: [32, 1]
value_specs:
  value_head_hidden_size: [32, 32, 1]
lr: 4.0e-3
weightdecay: 0.1
eps: 1.0e-4
value_pred_coef: 0.1
entropy_coef: 0.1
clip_epsilon: 0.2
max_num_iterations: 20
num_episodes_per_iteration: 5
max_sequence_length: 30
num_optim_epoch: 4
mini_batch_size: 256
save_model_interval: 1
