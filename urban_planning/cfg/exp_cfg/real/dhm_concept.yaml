# env
objectives_plan: objectives_dhm
init_plan: init_plan_dhm_concept

# reward
reward_specs:
  road_network_weight: 0.0
  life_circle_weight: 10.0
  greeness_weight: 1.0
  concept_weight: 1.0
  weight_by_area: true

# agent
agent_specs:
  batch_stage: false

# training parameters
skip_land_use: false
skip_road: true
road_ratio: 0.00
gamma: 2.0
tau: 0.0
state_encoder_specs:
  state_encoder_hidden_size: [64, 16]
  gcn_node_dim: 8
  num_gcn_layers: 4
  num_edge_fc_layers: 2
  max_num_nodes: 2000
  max_num_edges: 2000
  num_attention_heads: 2
policy_specs:
  policy_land_use_head_hidden_size: [32, 1]
  policy_road_head_hidden_size: [32, 1]
value_specs:
  value_head_hidden_size: [32, 32, 1]
lr: 4.0e-3
weightdecay: 0.0
eps: 1.0e-4
value_pred_coef: 0.1
entropy_coef: 0.1
clip_epsilon: 0.1
max_num_iterations: 5
num_episodes_per_iteration: 1
max_sequence_length: 50
num_optim_epoch: 4
mini_batch_size: 256
save_model_interval: 1
